import pandas as pd
import re
import os
import string
from googleapiclient.discovery import build
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
from urllib.parse import urlparse, parse_qs

# Download stopwords if not already present
nltk.download('stopwords')
nltk.download('punkt')

# Replace with your own YouTube Data API key
API_KEY = ''
youtube = build('youtube', 'v3', developerKey=API_KEY)

# Load BERT-based sentiment model
tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
labels = ['Negative', 'Neutral', 'Positive']

# Function to clean the text
def clean_text(text):
    text = re.sub(r'http\S+', '', text)  # remove URLs
    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # remove mentions
    text = re.sub(r'#[A-Za-z0-9_]+', '', text)  # remove hashtags
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)  # remove special characters
    text = text.lower().strip()  # convert to lowercase and strip whitespace
    text = remove_stopwords(text)
    return text

# Function to remove stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = ' '.join([word for word in word_tokens if word not in stop_words])
    return filtered_text

# Extract video ID from URL or return as is
def extract_video_id(input_str):
    if 'youtube.com' in input_str:
        url_parsed = urlparse(input_str)
        query_params = parse_qs(url_parsed.query)
        return query_params.get('v', [None])[0]
    elif 'youtu.be' in input_str:
        return input_str.split('/')[-1]
    else:
        return input_str

# Fetch comments from YouTube videos
def get_video_comments(video_id, max_comments=1000):
    comments = []
    next_page_token = None

    while len(comments) < max_comments:
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=100,
            pageToken=next_page_token
        )
        response = request.execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']
            cleaned_text = clean_text(comment['textDisplay'])
            comments.append({
                'author': comment['authorDisplayName'],
                'text': cleaned_text,
                'likes': comment['likeCount'],
                'publishedAt': comment['publishedAt']
            })

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break

    return pd.DataFrame(comments)

# Analyze sentiment using BERT-based model
def analyze_sentiment_bert(comment):
    inputs = tokenizer(comment, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    scores = outputs[0][0].detach().numpy()
    scores = torch.nn.functional.softmax(torch.tensor(scores), dim=0).numpy()
    sentiment = labels[np.argmax(scores)]
    return sentiment

if __name__ == "__main__":
    video_input = input("Enter YouTube video URL or ID: ")
    video_id = extract_video_id(video_input)
    print(f"Extracted Video ID: {video_id}")

    print("Fetching comments...")
    comments_df = get_video_comments(video_id)
    print(f"Fetched {len(comments_df)} comments.")

    print("Performing sentiment analysis with BERT model...")
    comments_df['sentiment'] = comments_df['text'].apply(analyze_sentiment_bert)

    # Save results to CSV
    output_file = f"youtube_comments_{video_id}_bert.csv"
    comments_df.to_csv(output_file, index=False)
    print(f"Saved to {output_file}")
